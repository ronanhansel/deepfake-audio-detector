{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732589001148,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "5sEGjg1roLzC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from scipy.signal import butter, lfilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import Input, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPrxGHEQtpuv"
   },
   "source": [
    "# GET SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1732586423355,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "S6Q9u-Tcy7W1"
   },
   "outputs": [],
   "source": [
    "num_of_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27666,
     "status": "ok",
     "timestamp": 1732586540405,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "ODAjzwJ0q9XP",
    "outputId": "be0b3070-bbf1-43ff-c1d6-f812c0cbc471"
   },
   "outputs": [],
   "source": [
    "# Function to get all .wav files from a directory\n",
    "def get_wav_files_from_folder(path):\n",
    "    return [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.wav')]\n",
    "\n",
    "# Function to load and display a .wav file\n",
    "def load_and_display_wav(file_path, num_of_samples):\n",
    "    try:\n",
    "        # Load the audio file using librosa\n",
    "        audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "        plt.title(f'Waveform of {os.path.basename(file_path)}')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return audio_data, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or displaying {file_path}: {e}\")\n",
    "\n",
    "# Function to randomly select files from the lists\n",
    "def select_random_files(real_files, fake_files, num_real, num_fake):\n",
    "    if not real_files:\n",
    "        print(\"No real files found!\")\n",
    "    if not fake_files:\n",
    "        print(\"No fake files found!\")\n",
    "\n",
    "    # Select random files from the real and fake lists\n",
    "    selected_real_files = random.sample(real_files, min(num_real, len(real_files)))\n",
    "    selected_fake_files = random.sample(fake_files, min(num_fake, len(fake_files)))\n",
    "\n",
    "    return selected_real_files, selected_fake_files\n",
    "\n",
    "# Paths to the real and fake directories (replace with your actual paths)\n",
    "real_path = \"./content/LibriSeVoc/diffwave\"\n",
    "fake_path = \"./content/LibriSeVoc/gt\"\n",
    "\n",
    "# Load the lists of .wav files from each directory\n",
    "real_files = get_wav_files_from_folder(real_path)\n",
    "fake_files = get_wav_files_from_folder(fake_path)\n",
    "\n",
    "# Get lists of 10 random real and fake file paths\n",
    "random_real_files, random_fake_files = select_random_files(real_files, fake_files, num_real=num_of_samples, num_fake=num_of_samples)\n",
    "\n",
    "Real_Audio = []\n",
    "Fake_Audio = []\n",
    "\n",
    "# # Display the real files\n",
    "# print(\"Displaying random real files:\")\n",
    "# for file in random_real_files:\n",
    "#     Real_Audio.append(load_and_display_wav(file, num_of_samples))\n",
    "\n",
    "# # Display the fake files\n",
    "# print(\"Displaying random fake files:\")\n",
    "# for file in random_fake_files:\n",
    "#     Fake_Audio.append(load_and_display_wav(file, num_of_samples))\n",
    "\n",
    "# print(len(Real_Audio))\n",
    "# print(len(Fake_Audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zeonzo6krBS2",
    "outputId": "36c119f3-4997-47b0-e63b-688f8f8a86ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Audio\n",
      "Fake Audio\n"
     ]
    }
   ],
   "source": [
    "def play_audio(audio_data_list):\n",
    "    for audio_data, sample_rate in audio_data_list:\n",
    "        print(f\"Playing audio with sample rate: {sample_rate} Hz\")\n",
    "        ipd.display(ipd.Audio(data=audio_data, rate=sample_rate))\n",
    "\n",
    "print(\"Real Audio\")\n",
    "play_audio(Real_Audio)\n",
    "print(\"Fake Audio\")\n",
    "play_audio(Fake_Audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h_6czCmrhAG"
   },
   "source": [
    "# *PREPROCESSING FUNCTION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available and set the device accordingly\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1732586434405,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "aeM3kWbZrDqH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Check MPS availability and set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def bandpass_filter(y, sr, lowcut=250, highcut=4000, order=5):\n",
    "    \"\"\"\n",
    "    Applies a bandpass filter to an audio signal.\n",
    "\n",
    "    Args:\n",
    "        y (torch.Tensor): The audio signal as a PyTorch tensor.\n",
    "        sr (int): The sample rate of the audio signal.\n",
    "        lowcut (int, optional): The lower cutoff frequency. Defaults to 250.\n",
    "        highcut (int, optional): The upper cutoff frequency. Defaults to 4000.\n",
    "        order (int, optional): The order of the filter. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The filtered audio signal as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    # Perform the filtering (this part uses scipy and will run on the CPU)\n",
    "    nyq = 0.5 * sr\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y_filtered = lfilter(b, a, y)  # Move to CPU for scipy\n",
    "\n",
    "    # Move the filtered signal back to the original device\n",
    "    return torch.tensor(y_filtered, dtype=y.dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732586435668,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "DB-gIQMss60B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def decrease_low_db(y, sr, threshold_db=-50, target_db=-80):\n",
    "    \"\"\"\n",
    "    Giảm độ lớn của các mẫu âm thanh dưới ngưỡng dB cho trước đến độ to mong muốn,\n",
    "    giữ nguyên thời gian của tín hiệu âm thanh.\n",
    "\n",
    "    :param y: Tín hiệu âm thanh (tensor)\n",
    "    :param sr: Tần số lấy mẫu (Hz)\n",
    "    :param threshold_db: Ngưỡng dB để xác định các mẫu cần giảm độ lớn (ví dụ: -40 dB)\n",
    "    :param target_db: Độ to mong muốn cho các mẫu dưới ngưỡng (ví dụ: -80 dB)\n",
    "    :return: Tín hiệu đã được điều chỉnh (tensor)\n",
    "    \"\"\"\n",
    "    # Calculate the absolute amplitude of the signal\n",
    "    abs_y = torch.abs(y)\n",
    "\n",
    "    # Calculate the reference amplitude (maximum amplitude)\n",
    "    ref_amplitude = torch.max(abs_y) if torch.max(abs_y) > 0 else torch.tensor(1.0, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Calculate the dB level of each sample relative to the reference amplitude\n",
    "    y_db = 20 * torch.log10(abs_y / ref_amplitude + 1e-10)  # Add epsilon to avoid log(0)\n",
    "\n",
    "    # Create a mask for samples below the dB threshold\n",
    "    mask = y_db < threshold_db\n",
    "\n",
    "    # Calculate the desired amplitude for samples below the dB threshold\n",
    "    desired_amplitude = 10 ** (target_db / 20) * ref_amplitude  # Example: -80 dB\n",
    "\n",
    "    # Create a copy of the signal to adjust\n",
    "    y_adjusted = y.clone()\n",
    "\n",
    "    # Reduce the amplitude of samples below the dB threshold\n",
    "    # Avoid division by zero by adding epsilon\n",
    "    y_adjusted[mask] = y_adjusted[mask] / (abs_y[mask] + 1e-10) * desired_amplitude\n",
    "\n",
    "    return y_adjusted  # Convert back to numpy array if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vvvdZeetmNg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MspLi1niuKFa"
   },
   "source": [
    "# SHOW PROCESSED INSTANCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CycaecXorgEO"
   },
   "outputs": [],
   "source": [
    "# Filtered_Real_Audio = []\n",
    "# for audio_data, sample_rate in Real_Audio:\n",
    "#     filtered_audio = bandpass_filter(audio_data, sample_rate, lowcut=250, highcut=4000)\n",
    "#     final_audio = decrease_low_db(filtered_audio, sample_rate)\n",
    "#     Filtered_Real_Audio.append(final_audio)\n",
    "\n",
    "# Filtered_Fake_Audio = []\n",
    "# for audio_data, sample_rate in Fake_Audio:\n",
    "#     filtered_audio = bandpass_filter(audio_data, sample_rate, lowcut=250, highcut=4000)\n",
    "#     filtered_audio = decrease_low_db(filtered_audio, sample_rate)\n",
    "#     Filtered_Fake_Audio.append(filtered_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rpqO-OR3xTuC",
    "outputId": "a286c7b2-0bc4-4fac-c848-1cb1033ff653"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def play_and_show_wave_spectrogram(audio_data, sample_rate, title):\n",
    "#     \"\"\"Plays audio, displays waveform, and spectrogram.\"\"\"\n",
    "#     ipd.display(ipd.Audio(data=audio_data, rate=sample_rate))  # Play audio\n",
    "\n",
    "#     # Display waveform\n",
    "#     # plt.figure(figsize=(10, 4))\n",
    "#     # librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "#     # plt.title(f\"{title} - Waveform\")\n",
    "#     # plt.xlabel(\"Time (s)\")\n",
    "#     # plt.ylabel(\"Amplitude\")\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n",
    "\n",
    "#     # Display spectrogram\n",
    "#     # plt.figure(figsize=(10, 4))\n",
    "#     # D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "#     # librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log')\n",
    "#     # plt.colorbar(format='%+2.0f dB')\n",
    "#     # plt.title(f\"{title} - Spectrogram\")\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n",
    "\n",
    "# # Play, display waveform, and spectrogram for Filtered_Real_Audio\n",
    "# print(\"Real Audio Display\")\n",
    "# for i, audio_data in enumerate(Filtered_Real_Audio):\n",
    "#     play_and_show_wave_spectrogram(audio_data, Real_Audio[i][1], f\"Filtered Real Audio {i+1}\")\n",
    "\n",
    "# # Play, display waveform, and spectrogram for Filtered_Fake_Audio\n",
    "# print(\"Fake Audio Display\")\n",
    "# for i, audio_data in enumerate(Filtered_Fake_Audio):\n",
    "#     play_and_show_wave_spectrogram(audio_data, Fake_Audio[i][1], f\"Filtered Fake Audio {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV62zb5qtg-L"
   },
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732586540406,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "D92vGPt6bENR",
    "outputId": "8a266eb6-9b0e-4d64-dd04-d089c6d32dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file paths: 21120\n",
      "Test file paths: 5282\n"
     ]
    }
   ],
   "source": [
    "# Define the main data directory\n",
    "\n",
    "# List to hold file paths\n",
    "train_file_paths = []\n",
    "test_file_paths = []\n",
    "\n",
    "label_dict = {\n",
    "    fake_path: 0,\n",
    "    real_path: 1\n",
    "}\n",
    "\n",
    "# Split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Split files in each class directory\n",
    "for class_dir in [fake_path, real_path]:\n",
    "\n",
    "    # Get all file paths for the class\n",
    "    all_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.wav')]\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_files, test_files = train_test_split(all_files, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    # Append to the respective lists with corresponding labels (class)\n",
    "    for file_path in train_files:\n",
    "        train_file_paths.append((file_path, label_dict[class_dir]))  # Store path and label\n",
    "    for file_path in test_files:\n",
    "        test_file_paths.append((file_path, label_dict[class_dir]))\n",
    "\n",
    "print(f\"Train file paths: {len(train_file_paths)}\")\n",
    "print(f\"Test file paths: {len(test_file_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1732586580946,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "cJYk9AcdF8bO",
    "outputId": "cebbd6cb-685c-4ba7-d758-c85d329b8b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file paths saved to output/train_file_paths.csv\n",
      "Test file paths saved to output/test_file_paths.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File paths to save the CSVs\n",
    "train_csv = 'output/train_file_paths.csv'\n",
    "test_csv = 'output/test_file_paths.csv'\n",
    "\n",
    "# Save train_file_paths to CSV\n",
    "with open(train_csv, mode='w', newline='') as train_file:\n",
    "    writer = csv.writer(train_file)\n",
    "    writer.writerow(['file_path', 'label'])  # Write the header\n",
    "    for file_path, label in train_file_paths:\n",
    "        writer.writerow([file_path, label])  # Write the file path and label\n",
    "\n",
    "# Save test_file_paths to CSV\n",
    "with open(test_csv, mode='w', newline='') as test_file:\n",
    "    writer = csv.writer(test_file)\n",
    "    writer.writerow(['file_path', 'label'])  # Write the header\n",
    "    for file_path, label in test_file_paths:\n",
    "        writer.writerow([file_path, label])  # Write the file path and label\n",
    "\n",
    "print(f\"Train file paths saved to {train_csv}\")\n",
    "print(f\"Test file paths saved to {test_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuvsA5gzF8bP"
   },
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1732588475361,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "lVT40_iHF8bP"
   },
   "outputs": [],
   "source": [
    "SEGMENT_LENGTH = 1\n",
    "NUM_SEGMENT = 30\n",
    "SR = 24000\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available and set the device accordingly\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1732586600090,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "WooQOFc5F8bP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Check MPS availability and set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def segment_to_spectrogram(segment, sr=24000, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Extracts a Mel spectrogram from an audio segment, ensuring execution on the MPS GPU if available.\n",
    "\n",
    "    Args:\n",
    "        segment (torch.Tensor): The audio segment as a PyTorch tensor.\n",
    "        sr (int, optional): The sample rate of the audio segment. Defaults to 24000.\n",
    "        n_fft (int, optional): The size of the FFT. Defaults to 2048.\n",
    "        hop_length (int, optional): The hop length for the STFT. Defaults to 512.\n",
    "        n_mels (int, optional): The number of Mel filterbanks. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Mel spectrogram in decibels (dB).\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the MelSpectrogram transform\n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    ).to(device)  # Ensure the transform is also on the correct device\n",
    "\n",
    "    # Apply the MelSpectrogram transform\n",
    "    mel_spectrogram = mel_spectrogram(segment)\n",
    "\n",
    "    # Convert to decibels (dB)\n",
    "    spectrogram_db = T.AmplitudeToDB().to(device)  # Move AmplitudeToDB to the device\n",
    "    spectrogram_db = spectrogram_db(mel_spectrogram)\n",
    "\n",
    "    return spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1370,
     "status": "ok",
     "timestamp": 1732586719872,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "F2ascfrZF8bP",
    "outputId": "9488f00d-7d2d-4637-cc3c-208264e83ce5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/d7p0zrqx7l3f7dwplj0g_b040000gn/T/ipykernel_43857/2508326885.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  waveform = torch.tensor(waveform, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 30 segments\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def extract_segments(audio_file, segment_length=SEGMENT_LENGTH, num_segments=NUM_SEGMENT):\n",
    "    # Load audio file using torchaudio\n",
    "    waveform, sr = torchaudio.load(audio_file)\n",
    "    waveform = bandpass_filter(waveform, sr, lowcut=250, highcut=4000)\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32).to(device)\n",
    "    waveform = decrease_low_db(waveform, sr)\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sr != SR:\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=SR)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = SR\n",
    "\n",
    "    # Calculate the total duration in seconds\n",
    "    total_duration = waveform.shape[1] / sr\n",
    "\n",
    "    # Calculate the overlap to ensure exactly num_segments\n",
    "    overlap = (total_duration - segment_length) / (num_segments - 1)\n",
    "\n",
    "    # Convert segment length and overlap to samples\n",
    "    segment_samples = int(segment_length * sr)\n",
    "    overlap_samples = int(overlap * sr)\n",
    "\n",
    "    # Extract the segments\n",
    "    segments = []\n",
    "    for i in range(num_segments):\n",
    "        start_sample = i * overlap_samples\n",
    "        end_sample = start_sample + segment_samples\n",
    "        segment = waveform[:, start_sample:end_sample]\n",
    "        spectrogram = segment_to_spectrogram(segment)\n",
    "        segments.append(spectrogram)\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Example usage\n",
    "audio_file = './content/LibriSeVoc/gt/19_227_000003_000000.wav'\n",
    "segments = extract_segments(audio_file)\n",
    "print(f\"Extracted {len(segments)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1732588501369,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "DAywUfvsF8bP",
    "outputId": "9f83ba35-702e-413b-f582-f3747dbb7f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              file_path  label\n",
      "0     ./content/LibriSeVoc/gt/60_121082_000096_00000...      0\n",
      "1     ./content/LibriSeVoc/gt/8312_279790_000004_000...      0\n",
      "2     ./content/LibriSeVoc/gt/3168_173564_000017_000...      0\n",
      "3     ./content/LibriSeVoc/gt/7302_86814_000053_0000...      0\n",
      "4     ./content/LibriSeVoc/gt/4813_248638_000012_000...      0\n",
      "...                                                 ...    ...\n",
      "1995  ./content/LibriSeVoc/diffwave/1116_132851_0000...      1\n",
      "1996  ./content/LibriSeVoc/diffwave/87_121553_000086...      1\n",
      "1997  ./content/LibriSeVoc/diffwave/2002_139469_0000...      1\n",
      "1998  ./content/LibriSeVoc/diffwave/1088_129236_0000...      1\n",
      "1999  ./content/LibriSeVoc/diffwave/3168_173565_0000...      1\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV containing train and validation file paths and labels\n",
    "train_csv = './output/train_file_paths.csv'  # Path to the train data CSV\n",
    "\n",
    "train_data = pd.read_csv(train_csv)\n",
    "\n",
    "train_data_head = train_data.head(1000)\n",
    "train_data_tail = train_data.tail(1000)\n",
    "train_data_tail = train_data_tail.reset_index(drop=True)\n",
    "train_data_head = train_data_head.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Concatenate the head and tail along rows (axis=0)\n",
    "demo_train_data = pd.concat([train_data_head, train_data_tail], axis=0)\n",
    "demo_train_data = demo_train_data.reset_index(drop=True)\n",
    "\n",
    "# Display the merged data\n",
    "print(demo_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495854,
     "status": "ok",
     "timestamp": 1732588999288,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "1zsxc37kF8bQ",
    "outputId": "ecace7f8-593e-4aef-c51b-7986f039fd40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and saving train data:  21120\n",
      "Resuming from index 21200\n",
      "Already finished processing. Merging.\n",
      "Loading train_data_partial_19700.pt\n",
      "Loaded 100 segments from train_data_partial_19700.pt\n",
      "Loading train_data_partial_17800.pt\n",
      "Loaded 100 segments from train_data_partial_17800.pt\n",
      "Loading train_data_partial_8600.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/d7p0zrqx7l3f7dwplj0g_b040000gn/T/ipykernel_43857/562766099.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  partial_data = torch.load(os.path.join(os.path.dirname(save_path), partial_file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 segments from train_data_partial_8600.pt\n",
      "Loading train_data_partial_20600.pt\n",
      "Loaded 100 segments from train_data_partial_20600.pt\n",
      "Loading train_data_partial_6900.pt\n",
      "Loaded 100 segments from train_data_partial_6900.pt\n",
      "Loading train_data_partial_13900.pt\n",
      "Loaded 100 segments from train_data_partial_13900.pt\n",
      "Loading train_data_partial_21000.pt\n",
      "Loaded 100 segments from train_data_partial_21000.pt\n",
      "Loading train_data_partial_9000.pt\n",
      "Loaded 100 segments from train_data_partial_9000.pt\n",
      "Loading train_data_partial_18100.pt\n",
      "Loaded 100 segments from train_data_partial_18100.pt\n",
      "Loading train_data_partial_2800.pt\n",
      "Loaded 100 segments from train_data_partial_2800.pt\n",
      "Loading train_data_partial_11500.pt\n",
      "Loaded 100 segments from train_data_partial_11500.pt\n",
      "Loading train_data_partial_5300.pt\n",
      "Loaded 100 segments from train_data_partial_5300.pt\n",
      "Loading train_data_partial_14200.pt\n",
      "Loaded 100 segments from train_data_partial_14200.pt\n",
      "Loading train_data_partial_15400.pt\n",
      "Loaded 100 segments from train_data_partial_15400.pt\n",
      "Loading train_data_partial_1200.pt\n",
      "Loaded 100 segments from train_data_partial_1200.pt\n",
      "Loading train_data_partial_10300.pt\n",
      "Loaded 100 segments from train_data_partial_10300.pt\n",
      "Loading train_data_partial_4500.pt\n",
      "Loaded 100 segments from train_data_partial_4500.pt\n",
      "Loading train_data_partial_3200.pt\n",
      "Loaded 100 segments from train_data_partial_3200.pt\n",
      "Loading train_data_partial_17400.pt\n",
      "Loaded 100 segments from train_data_partial_17400.pt\n",
      "Loading train_data_partial_6500.pt\n",
      "Loaded 100 segments from train_data_partial_6500.pt\n",
      "Loading train_data_partial_12300.pt\n",
      "Loaded 100 segments from train_data_partial_12300.pt\n",
      "Loading train_data_partial_7300.pt\n",
      "Loaded 100 segments from train_data_partial_7300.pt\n",
      "Loading train_data_partial_13500.pt\n",
      "Loaded 100 segments from train_data_partial_13500.pt\n",
      "Loading train_data_partial_2400.pt\n",
      "Loaded 100 segments from train_data_partial_2400.pt\n",
      "Loading train_data_partial_16200.pt\n",
      "Loaded 100 segments from train_data_partial_16200.pt\n",
      "Loading train_data_partial_11900.pt\n",
      "Loaded 100 segments from train_data_partial_11900.pt\n",
      "Loading train_data_partial_15800.pt\n",
      "Loaded 100 segments from train_data_partial_15800.pt\n",
      "Loading train_data_partial_700.pt\n",
      "Loaded 100 segments from train_data_partial_700.pt\n",
      "Loading train_data_partial_4900.pt\n",
      "Loaded 100 segments from train_data_partial_4900.pt\n",
      "Loading train_data_partial_4800.pt\n",
      "Loaded 100 segments from train_data_partial_4800.pt\n",
      "Loading train_data_partial_15900.pt\n",
      "Loaded 100 segments from train_data_partial_15900.pt\n",
      "Loading train_data_partial_600.pt\n",
      "Loaded 100 segments from train_data_partial_600.pt\n",
      "Loading train_data_partial_21120.pt\n",
      "Loaded 20 segments from train_data_partial_21120.pt\n",
      "Loading train_data_partial_11800.pt\n",
      "Loaded 100 segments from train_data_partial_11800.pt\n",
      "Loading train_data_partial_16300.pt\n",
      "Loaded 100 segments from train_data_partial_16300.pt\n",
      "Loading train_data_partial_2500.pt\n",
      "Loaded 100 segments from train_data_partial_2500.pt\n",
      "Loading train_data_partial_13400.pt\n",
      "Loaded 100 segments from train_data_partial_13400.pt\n",
      "Loading train_data_partial_7200.pt\n",
      "Loaded 100 segments from train_data_partial_7200.pt\n",
      "Loading train_data_partial_12200.pt\n",
      "Loaded 100 segments from train_data_partial_12200.pt\n",
      "Loading train_data_partial_6400.pt\n",
      "Loaded 100 segments from train_data_partial_6400.pt\n",
      "Loading train_data_partial_17500.pt\n",
      "Loaded 100 segments from train_data_partial_17500.pt\n",
      "Loading train_data_partial_3300.pt\n",
      "Loaded 100 segments from train_data_partial_3300.pt\n",
      "Loading train_data_partial_4400.pt\n",
      "Loaded 100 segments from train_data_partial_4400.pt\n",
      "Loading train_data_partial_10200.pt\n",
      "Loaded 100 segments from train_data_partial_10200.pt\n",
      "Loading train_data_partial_1300.pt\n",
      "Loaded 100 segments from train_data_partial_1300.pt\n",
      "Loading train_data_partial_15500.pt\n",
      "Loaded 100 segments from train_data_partial_15500.pt\n",
      "Loading train_data_partial_14300.pt\n",
      "Loaded 100 segments from train_data_partial_14300.pt\n",
      "Loading train_data_partial_5200.pt\n",
      "Loaded 100 segments from train_data_partial_5200.pt\n",
      "Loading train_data_partial_11400.pt\n",
      "Loaded 100 segments from train_data_partial_11400.pt\n",
      "Loading train_data_partial_2900.pt\n",
      "Loaded 100 segments from train_data_partial_2900.pt\n",
      "Loading train_data_partial_18000.pt\n",
      "Loaded 100 segments from train_data_partial_18000.pt\n",
      "Loading train_data_partial_21100.pt\n",
      "Loaded 100 segments from train_data_partial_21100.pt\n",
      "Loading train_data_partial_9100.pt\n",
      "Loaded 100 segments from train_data_partial_9100.pt\n",
      "Loading train_data_partial_13800.pt\n",
      "Loaded 100 segments from train_data_partial_13800.pt\n",
      "Loading train_data_partial_8700.pt\n",
      "Loaded 100 segments from train_data_partial_8700.pt\n",
      "Loading train_data_partial_20700.pt\n",
      "Loaded 100 segments from train_data_partial_20700.pt\n",
      "Loading train_data_partial_6800.pt\n",
      "Loaded 100 segments from train_data_partial_6800.pt\n",
      "Loading train_data_partial_19600.pt\n",
      "Loaded 100 segments from train_data_partial_19600.pt\n",
      "Loading train_data_partial_17900.pt\n",
      "Loaded 100 segments from train_data_partial_17900.pt\n",
      "Loading train_data_partial_3400.pt\n",
      "Loaded 100 segments from train_data_partial_3400.pt\n",
      "Loading train_data_partial_17200.pt\n",
      "Loaded 100 segments from train_data_partial_17200.pt\n",
      "Loading train_data_partial_6300.pt\n",
      "Loaded 100 segments from train_data_partial_6300.pt\n",
      "Loading train_data_partial_12500.pt\n",
      "Loaded 100 segments from train_data_partial_12500.pt\n",
      "Loading train_data_partial_7500.pt\n",
      "Loaded 100 segments from train_data_partial_7500.pt\n",
      "Loading train_data_partial_13300.pt\n",
      "Loaded 100 segments from train_data_partial_13300.pt\n",
      "Loading train_data_partial_2200.pt\n",
      "Loaded 100 segments from train_data_partial_2200.pt\n",
      "Loading train_data_partial_16400.pt\n",
      "Loaded 100 segments from train_data_partial_16400.pt\n",
      "Loading train_data_partial_5900.pt\n",
      "Loaded 100 segments from train_data_partial_5900.pt\n",
      "Loading train_data_partial_14800.pt\n",
      "Loaded 100 segments from train_data_partial_14800.pt\n",
      "Loading train_data_partial_1800.pt\n",
      "Loaded 100 segments from train_data_partial_1800.pt\n",
      "Loading train_data_partial_100.pt\n",
      "Loaded 100 segments from train_data_partial_100.pt\n",
      "Loading train_data_partial_10900.pt\n",
      "Loaded 100 segments from train_data_partial_10900.pt\n",
      "Loading train_data_partial_19100.pt\n",
      "Loaded 100 segments from train_data_partial_19100.pt\n",
      "Loading train_data_partial_3800.pt\n",
      "Loaded 100 segments from train_data_partial_3800.pt\n",
      "Loading train_data_partial_12900.pt\n",
      "Loaded 100 segments from train_data_partial_12900.pt\n",
      "Loading train_data_partial_20000.pt\n",
      "Loaded 100 segments from train_data_partial_20000.pt\n",
      "Loading train_data_partial_8000.pt\n",
      "Loaded 100 segments from train_data_partial_8000.pt\n",
      "Loading train_data_partial_9600.pt\n",
      "Loaded 100 segments from train_data_partial_9600.pt\n",
      "Loading train_data_partial_7900.pt\n",
      "Loaded 100 segments from train_data_partial_7900.pt\n",
      "Loading train_data_partial_18700.pt\n",
      "Loaded 100 segments from train_data_partial_18700.pt\n",
      "Loading train_data_partial_16800.pt\n",
      "Loaded 100 segments from train_data_partial_16800.pt\n",
      "Loading train_data_partial_11300.pt\n",
      "Loaded 100 segments from train_data_partial_11300.pt\n",
      "Loading train_data_partial_5500.pt\n",
      "Loaded 100 segments from train_data_partial_5500.pt\n",
      "Loading train_data_partial_14400.pt\n",
      "Loaded 100 segments from train_data_partial_14400.pt\n",
      "Loading train_data_partial_15200.pt\n",
      "Loaded 100 segments from train_data_partial_15200.pt\n",
      "Loading train_data_partial_1400.pt\n",
      "Loaded 100 segments from train_data_partial_1400.pt\n",
      "Loading train_data_partial_10500.pt\n",
      "Loaded 100 segments from train_data_partial_10500.pt\n",
      "Loading train_data_partial_4300.pt\n",
      "Loaded 100 segments from train_data_partial_4300.pt\n",
      "Loading train_data_partial_4200.pt\n",
      "Loaded 100 segments from train_data_partial_4200.pt\n",
      "Loading train_data_partial_10400.pt\n",
      "Loaded 100 segments from train_data_partial_10400.pt\n",
      "Loading train_data_partial_1500.pt\n",
      "Loaded 100 segments from train_data_partial_1500.pt\n",
      "Loading train_data_partial_15300.pt\n",
      "Loaded 100 segments from train_data_partial_15300.pt\n",
      "Loading train_data_partial_14500.pt\n",
      "Loaded 100 segments from train_data_partial_14500.pt\n",
      "Loading train_data_partial_5400.pt\n",
      "Loaded 100 segments from train_data_partial_5400.pt\n",
      "Loading train_data_partial_11200.pt\n",
      "Loaded 100 segments from train_data_partial_11200.pt\n",
      "Loading train_data_partial_18600.pt\n",
      "Loaded 100 segments from train_data_partial_18600.pt\n",
      "Loading train_data_partial_16900.pt\n",
      "Loaded 100 segments from train_data_partial_16900.pt\n",
      "Loading train_data_partial_9700.pt\n",
      "Loaded 100 segments from train_data_partial_9700.pt\n",
      "Loading train_data_partial_7800.pt\n",
      "Loaded 100 segments from train_data_partial_7800.pt\n",
      "Loading train_data_partial_20100.pt\n",
      "Loaded 100 segments from train_data_partial_20100.pt\n",
      "Loading train_data_partial_8100.pt\n",
      "Loaded 100 segments from train_data_partial_8100.pt\n",
      "Loading train_data_partial_12800.pt\n",
      "Loaded 100 segments from train_data_partial_12800.pt\n",
      "Loading train_data_partial_3900.pt\n",
      "Loaded 100 segments from train_data_partial_3900.pt\n",
      "Loading train_data_partial_19000.pt\n",
      "Loaded 100 segments from train_data_partial_19000.pt\n",
      "Loading train_data_partial_10800.pt\n",
      "Loaded 100 segments from train_data_partial_10800.pt\n",
      "Loading train_data_partial_1900.pt\n",
      "Loaded 100 segments from train_data_partial_1900.pt\n",
      "Loading train_data_partial_14900.pt\n",
      "Loaded 100 segments from train_data_partial_14900.pt\n",
      "Loading train_data_partial_5800.pt\n",
      "Loaded 100 segments from train_data_partial_5800.pt\n",
      "Loading train_data_partial_16500.pt\n",
      "Loaded 100 segments from train_data_partial_16500.pt\n",
      "Loading train_data_partial_2300.pt\n",
      "Loaded 100 segments from train_data_partial_2300.pt\n",
      "Loading train_data_partial_13200.pt\n",
      "Loaded 100 segments from train_data_partial_13200.pt\n",
      "Loading train_data_partial_7400.pt\n",
      "Loaded 100 segments from train_data_partial_7400.pt\n",
      "Loading train_data_partial_12400.pt\n",
      "Loaded 100 segments from train_data_partial_12400.pt\n",
      "Loading train_data_partial_6200.pt\n",
      "Loaded 100 segments from train_data_partial_6200.pt\n",
      "Loading train_data_partial_17300.pt\n",
      "Loaded 100 segments from train_data_partial_17300.pt\n",
      "Loading train_data_partial_3500.pt\n",
      "Loaded 100 segments from train_data_partial_3500.pt\n",
      "Loading train_data_partial_300.pt\n",
      "Loaded 100 segments from train_data_partial_300.pt\n",
      "Loading train_data_partial_16600.pt\n",
      "Loaded 100 segments from train_data_partial_16600.pt\n",
      "Loading train_data_partial_18900.pt\n",
      "Loaded 100 segments from train_data_partial_18900.pt\n",
      "Loading train_data_partial_2000.pt\n",
      "Loaded 100 segments from train_data_partial_2000.pt\n",
      "Loading train_data_partial_13100.pt\n",
      "Loaded 100 segments from train_data_partial_13100.pt\n",
      "Loading train_data_partial_7700.pt\n",
      "Loaded 100 segments from train_data_partial_7700.pt\n",
      "Loading train_data_partial_9800.pt\n",
      "Loaded 100 segments from train_data_partial_9800.pt\n",
      "Loading train_data_partial_12700.pt\n",
      "Loaded 100 segments from train_data_partial_12700.pt\n",
      "Loading train_data_partial_6100.pt\n",
      "Loaded 100 segments from train_data_partial_6100.pt\n",
      "Loading train_data_partial_17000.pt\n",
      "Loaded 100 segments from train_data_partial_17000.pt\n",
      "Loading train_data_partial_3600.pt\n",
      "Loaded 100 segments from train_data_partial_3600.pt\n",
      "Loading train_data_partial_4100.pt\n",
      "Loaded 100 segments from train_data_partial_4100.pt\n",
      "Loading train_data_partial_10700.pt\n",
      "Loaded 100 segments from train_data_partial_10700.pt\n",
      "Loading train_data_partial_1600.pt\n",
      "Loaded 100 segments from train_data_partial_1600.pt\n",
      "Loading train_data_partial_15000.pt\n",
      "Loaded 100 segments from train_data_partial_15000.pt\n",
      "Loading train_data_partial_14600.pt\n",
      "Loaded 100 segments from train_data_partial_14600.pt\n",
      "Loading train_data_partial_5700.pt\n",
      "Loaded 100 segments from train_data_partial_5700.pt\n",
      "Loading train_data_partial_11100.pt\n",
      "Loaded 100 segments from train_data_partial_11100.pt\n",
      "Loading train_data_partial_18500.pt\n",
      "Loaded 100 segments from train_data_partial_18500.pt\n",
      "Loading train_data_partial_9400.pt\n",
      "Loaded 100 segments from train_data_partial_9400.pt\n",
      "Loading train_data_partial_8200.pt\n",
      "Loaded 100 segments from train_data_partial_8200.pt\n",
      "Loading train_data_partial_20200.pt\n",
      "Loaded 100 segments from train_data_partial_20200.pt\n",
      "Loading train_data_partial_19300.pt\n",
      "Loaded 100 segments from train_data_partial_19300.pt\n",
      "Loading train_data_partial_19200.pt\n",
      "Loaded 100 segments from train_data_partial_19200.pt\n",
      "Loading train_data_partial_8300.pt\n",
      "Loaded 100 segments from train_data_partial_8300.pt\n",
      "Loading train_data_partial_20300.pt\n",
      "Loaded 100 segments from train_data_partial_20300.pt\n",
      "Loading train_data_partial_9500.pt\n",
      "Loaded 100 segments from train_data_partial_9500.pt\n",
      "Loading train_data_partial_18400.pt\n",
      "Loaded 100 segments from train_data_partial_18400.pt\n",
      "Loading train_data_partial_11000.pt\n",
      "Loaded 100 segments from train_data_partial_11000.pt\n",
      "Loading train_data_partial_5600.pt\n",
      "Loaded 100 segments from train_data_partial_5600.pt\n",
      "Loading train_data_partial_14700.pt\n",
      "Loaded 100 segments from train_data_partial_14700.pt\n",
      "Loading train_data_partial_15100.pt\n",
      "Loaded 100 segments from train_data_partial_15100.pt\n",
      "Loading train_data_partial_1700.pt\n",
      "Loaded 100 segments from train_data_partial_1700.pt\n",
      "Loading train_data_partial_10600.pt\n",
      "Loaded 100 segments from train_data_partial_10600.pt\n",
      "Loading train_data_partial_4000.pt\n",
      "Loaded 100 segments from train_data_partial_4000.pt\n",
      "Loading train_data_partial_3700.pt\n",
      "Loaded 100 segments from train_data_partial_3700.pt\n",
      "Loading train_data_partial_17100.pt\n",
      "Loaded 100 segments from train_data_partial_17100.pt\n",
      "Loading train_data_partial_6000.pt\n",
      "Loaded 100 segments from train_data_partial_6000.pt\n",
      "Loading train_data_partial_12600.pt\n",
      "Loaded 100 segments from train_data_partial_12600.pt\n",
      "Loading train_data_partial_7600.pt\n",
      "Loaded 100 segments from train_data_partial_7600.pt\n",
      "Loading train_data_partial_9900.pt\n",
      "Loaded 100 segments from train_data_partial_9900.pt\n",
      "Loading train_data_partial_13000.pt\n",
      "Loaded 100 segments from train_data_partial_13000.pt\n",
      "Loading train_data_partial_2100.pt\n",
      "Loaded 100 segments from train_data_partial_2100.pt\n",
      "Loading train_data_partial_16700.pt\n",
      "Loaded 100 segments from train_data_partial_16700.pt\n",
      "Loading train_data_partial_18800.pt\n",
      "Loaded 100 segments from train_data_partial_18800.pt\n",
      "Loading train_data_partial_200.pt\n",
      "Loaded 100 segments from train_data_partial_200.pt\n",
      "Loading train_data_partial_4700.pt\n",
      "Loaded 100 segments from train_data_partial_4700.pt\n",
      "Loading train_data_partial_10100.pt\n",
      "Loaded 100 segments from train_data_partial_10100.pt\n",
      "Loading train_data_partial_1000.pt\n",
      "Loaded 100 segments from train_data_partial_1000.pt\n",
      "Loading train_data_partial_900.pt\n",
      "Loaded 100 segments from train_data_partial_900.pt\n",
      "Loading train_data_partial_15600.pt\n",
      "Loaded 100 segments from train_data_partial_15600.pt\n",
      "Loading train_data_partial_14000.pt\n",
      "Loaded 100 segments from train_data_partial_14000.pt\n",
      "Loading train_data_partial_5100.pt\n",
      "Loaded 100 segments from train_data_partial_5100.pt\n",
      "Loading train_data_partial_11700.pt\n",
      "Loaded 100 segments from train_data_partial_11700.pt\n",
      "Loading train_data_partial_18300.pt\n",
      "Loaded 100 segments from train_data_partial_18300.pt\n",
      "Loading train_data_partial_9200.pt\n",
      "Loaded 100 segments from train_data_partial_9200.pt\n",
      "Loading train_data_partial_20400.pt\n",
      "Loaded 100 segments from train_data_partial_20400.pt\n",
      "Loading train_data_partial_8400.pt\n",
      "Loaded 100 segments from train_data_partial_8400.pt\n",
      "Loading train_data_partial_19500.pt\n",
      "Loaded 100 segments from train_data_partial_19500.pt\n",
      "Loading train_data_partial_500.pt\n",
      "Loaded 100 segments from train_data_partial_500.pt\n",
      "Loading train_data_partial_16000.pt\n",
      "Loaded 100 segments from train_data_partial_16000.pt\n",
      "Loading train_data_partial_2600.pt\n",
      "Loaded 100 segments from train_data_partial_2600.pt\n",
      "Loading train_data_partial_13700.pt\n",
      "Loaded 100 segments from train_data_partial_13700.pt\n",
      "Loading train_data_partial_7100.pt\n",
      "Loaded 100 segments from train_data_partial_7100.pt\n",
      "Loading train_data_partial_12100.pt\n",
      "Loaded 100 segments from train_data_partial_12100.pt\n",
      "Loading train_data_partial_6700.pt\n",
      "Loaded 100 segments from train_data_partial_6700.pt\n",
      "Loading train_data_partial_8800.pt\n",
      "Loaded 100 segments from train_data_partial_8800.pt\n",
      "Loading train_data_partial_20800.pt\n",
      "Loaded 100 segments from train_data_partial_20800.pt\n",
      "Loading train_data_partial_17600.pt\n",
      "Loaded 100 segments from train_data_partial_17600.pt\n",
      "Loading train_data_partial_19900.pt\n",
      "Loaded 100 segments from train_data_partial_19900.pt\n",
      "Loading train_data_partial_3000.pt\n",
      "Loaded 100 segments from train_data_partial_3000.pt\n",
      "Loading train_data_partial_3100.pt\n",
      "Loaded 100 segments from train_data_partial_3100.pt\n",
      "Loading train_data_partial_17700.pt\n",
      "Loaded 100 segments from train_data_partial_17700.pt\n",
      "Loading train_data_partial_19800.pt\n",
      "Loaded 100 segments from train_data_partial_19800.pt\n",
      "Loading train_data_partial_6600.pt\n",
      "Loaded 100 segments from train_data_partial_6600.pt\n",
      "Loading train_data_partial_8900.pt\n",
      "Loaded 100 segments from train_data_partial_8900.pt\n",
      "Loading train_data_partial_20900.pt\n",
      "Loaded 100 segments from train_data_partial_20900.pt\n",
      "Loading train_data_partial_12000.pt\n",
      "Loaded 100 segments from train_data_partial_12000.pt\n",
      "Loading train_data_partial_7000.pt\n",
      "Loaded 100 segments from train_data_partial_7000.pt\n",
      "Loading train_data_partial_13600.pt\n",
      "Loaded 100 segments from train_data_partial_13600.pt\n",
      "Loading train_data_partial_2700.pt\n",
      "Loaded 100 segments from train_data_partial_2700.pt\n",
      "Loading train_data_partial_16100.pt\n",
      "Loaded 100 segments from train_data_partial_16100.pt\n",
      "Loading train_data_partial_400.pt\n",
      "Loaded 100 segments from train_data_partial_400.pt\n",
      "Loading train_data_partial_19400.pt\n",
      "Loaded 100 segments from train_data_partial_19400.pt\n",
      "Loading train_data_partial_20500.pt\n",
      "Loaded 100 segments from train_data_partial_20500.pt\n",
      "Loading train_data_partial_8500.pt\n",
      "Loaded 100 segments from train_data_partial_8500.pt\n",
      "Loading train_data_partial_9300.pt\n",
      "Loaded 100 segments from train_data_partial_9300.pt\n",
      "Loading train_data_partial_18200.pt\n",
      "Loaded 100 segments from train_data_partial_18200.pt\n",
      "Loading train_data_partial_11600.pt\n",
      "Loaded 100 segments from train_data_partial_11600.pt\n",
      "Loading train_data_partial_5000.pt\n",
      "Loaded 100 segments from train_data_partial_5000.pt\n",
      "Loading train_data_partial_14100.pt\n",
      "Loaded 100 segments from train_data_partial_14100.pt\n",
      "Loading train_data_partial_800.pt\n",
      "Loaded 100 segments from train_data_partial_800.pt\n",
      "Loading train_data_partial_15700.pt\n",
      "Loaded 100 segments from train_data_partial_15700.pt\n",
      "Loading train_data_partial_1100.pt\n",
      "Loaded 100 segments from train_data_partial_1100.pt\n",
      "Loading train_data_partial_10000.pt\n",
      "Loaded 100 segments from train_data_partial_10000.pt\n",
      "Loading train_data_partial_4600.pt\n",
      "Loaded 100 segments from train_data_partial_4600.pt\n",
      "Completed loading train data. Saving...\n",
      "Train segments shape: (21120, 30, 128, 47, 1), Train labels shape: (21120,)\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = './output/train_data_checkpoint/train_data_partial'\n",
    "SAVE_INTERVAL = 100\n",
    "\n",
    "def load_data(data, segment_length=SEGMENT_LENGTH, num_segments=NUM_SEGMENT, save_interval=SAVE_INTERVAL, save_path=SAVE_PATH, start_index=0):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    # Check for existing partial files\n",
    "    partial_files = [f for f in os.listdir(os.path.dirname(save_path)) if f.startswith(os.path.basename(save_path)) and f.endswith('.pt')]\n",
    "    num_partial_files = len(partial_files)\n",
    "    if start_index == 0:\n",
    "        start_index = num_partial_files * save_interval\n",
    "    else:\n",
    "        start_index = start_index * save_interval\n",
    "    print(f\"Resuming from index {start_index}\")\n",
    "\n",
    "    if num_partial_files * save_interval >= len(data): # Check if all files have been processed\n",
    "        print(\"Already finished processing. Merging.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Continuing processing from the last checkpoint.\")\n",
    "\n",
    "    for idx, (_, row) in tqdm(enumerate(data.iterrows()), total=len(data)):\n",
    "        if idx < start_index:\n",
    "            continue\n",
    "\n",
    "        file_path = row['file_path']\n",
    "        label = row['label']\n",
    "\n",
    "        # Extract segments from the audio file\n",
    "        file_segments = extract_segments(file_path, segment_length, num_segments)\n",
    "\n",
    "        # Stack the segments and convert to torch tensor\n",
    "        file_segments_stacked = torch.stack([torch.tensor(segment, dtype=torch.float32) for segment in file_segments])\n",
    "\n",
    "        # Append the stacked segments and their corresponding labels\n",
    "        segments.append(file_segments_stacked)  # Append the 30 segments as a single element\n",
    "        labels.append(label)  # Append the label only once per file\n",
    "\n",
    "        # Save progress every save_interval\n",
    "        if (idx + 1) % save_interval == 0 or (idx + 1) == len(data):\n",
    "            partial_path = f\"{save_path}_{int((idx + 1))}.pt\"\n",
    "            torch.save({'segments': segments, 'labels': labels}, partial_path)\n",
    "            print(f\"{len(segments)} saved at index {idx + 1} to {partial_path}\")  # More informative print\n",
    "            print({'segments': segments[0], 'labels': labels[0]}) # Check the saved data\n",
    "            segments = []\n",
    "            labels = []\n",
    "\n",
    "    print(\"All segments saved individually.\")\n",
    "\n",
    "def load_saved_segments(data, save_path=SAVE_PATH):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Load all partial files in the SAVE_PATH subdir\n",
    "    partial_files = [f for f in os.listdir(os.path.dirname(save_path)) if f.startswith(os.path.basename(save_path)) and f.endswith('.pt')]\n",
    "    for partial_file in partial_files:\n",
    "        print(f\"Loading {partial_file}\")\n",
    "        partial_data = torch.load(os.path.join(os.path.dirname(save_path), partial_file))\n",
    "        print(f\"Loaded {len(partial_data['segments'])} segments from {partial_file}\")\n",
    "        all_segments.extend(partial_data['segments'])  # Extend directly with the list of stacked segments\n",
    "        all_labels.extend(partial_data['labels'])  # Extend the labels list\n",
    "    print(\"Finished loading, stacking loaded tensors...\")\n",
    "    return torch.stack([segment.cpu() for segment in all_segments]), torch.tensor(all_labels, dtype=torch.float32).cpu()\n",
    "\n",
    "\n",
    "if os.path.exists(SAVE_PATH + '.pt'):\n",
    "    # Load train and validation data\n",
    "    data = torch.load(SAVE_PATH + '.pt')\n",
    "    train_segments = data['segments']\n",
    "    print(f\"Loaded {len(train_segments)} segments\")\n",
    "    train_labels = data['labels']\n",
    "else:\n",
    "    print(\"Loading and saving train data: \", len(train_data))\n",
    "    # Load the train and validation data\n",
    "    load_data(train_data)  # Save segments individually\n",
    "    train_segments, train_labels = load_saved_segments(train_data)  # Load and combine saved segments\n",
    "    print(\"Completed loading train data. Saving...\")\n",
    "    # Save the train and validation data\n",
    "    torch.save({'segments': train_segments, 'labels': train_labels}, f'{SAVE_PATH}.pt')\n",
    "\n",
    "train_segments = np.array([segment.cpu().numpy() for segment in train_segments])\n",
    "train_segments = np.transpose(train_segments, (0, 1, 3, 4, 2))\n",
    "train_labels = train_labels.cpu().numpy().astype(int)\n",
    "print(f\"Train segments shape: {train_segments.shape}, Train labels shape: {train_labels.shape}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
