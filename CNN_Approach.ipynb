{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732589001148,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "5sEGjg1roLzC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from scipy.signal import butter, lfilter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import Input, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPrxGHEQtpuv"
   },
   "source": [
    "# GET SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1732586423355,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "S6Q9u-Tcy7W1"
   },
   "outputs": [],
   "source": [
    "num_of_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27666,
     "status": "ok",
     "timestamp": 1732586540405,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "ODAjzwJ0q9XP",
    "outputId": "be0b3070-bbf1-43ff-c1d6-f812c0cbc471"
   },
   "outputs": [],
   "source": [
    "# Function to get all .wav files from a directory\n",
    "def get_wav_files_from_folder(path):\n",
    "    return [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.wav')]\n",
    "\n",
    "# Function to load and display a .wav file\n",
    "def load_and_display_wav(file_path, num_of_samples):\n",
    "    try:\n",
    "        # Load the audio file using librosa\n",
    "        audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Plot the waveform\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "        plt.title(f'Waveform of {os.path.basename(file_path)}')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return audio_data, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or displaying {file_path}: {e}\")\n",
    "\n",
    "# Function to randomly select files from the lists\n",
    "def select_random_files(real_files, fake_files, num_real, num_fake):\n",
    "    if not real_files:\n",
    "        print(\"No real files found!\")\n",
    "    if not fake_files:\n",
    "        print(\"No fake files found!\")\n",
    "\n",
    "    # Select random files from the real and fake lists\n",
    "    selected_real_files = random.sample(real_files, min(num_real, len(real_files)))\n",
    "    selected_fake_files = random.sample(fake_files, min(num_fake, len(fake_files)))\n",
    "\n",
    "    return selected_real_files, selected_fake_files\n",
    "\n",
    "# Paths to the real and fake directories (replace with your actual paths)\n",
    "real_path = \"./content/LibriSeVoc/diffwave\"\n",
    "fake_path = \"./content/LibriSeVoc/gt\"\n",
    "\n",
    "# Load the lists of .wav files from each directory\n",
    "real_files = get_wav_files_from_folder(real_path)\n",
    "fake_files = get_wav_files_from_folder(fake_path)\n",
    "\n",
    "# Get lists of 10 random real and fake file paths\n",
    "random_real_files, random_fake_files = select_random_files(real_files, fake_files, num_real=num_of_samples, num_fake=num_of_samples)\n",
    "\n",
    "Real_Audio = []\n",
    "Fake_Audio = []\n",
    "\n",
    "# # Display the real files\n",
    "# print(\"Displaying random real files:\")\n",
    "# for file in random_real_files:\n",
    "#     Real_Audio.append(load_and_display_wav(file, num_of_samples))\n",
    "\n",
    "# # Display the fake files\n",
    "# print(\"Displaying random fake files:\")\n",
    "# for file in random_fake_files:\n",
    "#     Fake_Audio.append(load_and_display_wav(file, num_of_samples))\n",
    "\n",
    "# print(len(Real_Audio))\n",
    "# print(len(Fake_Audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Zeonzo6krBS2",
    "outputId": "36c119f3-4997-47b0-e63b-688f8f8a86ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Audio\n",
      "Fake Audio\n"
     ]
    }
   ],
   "source": [
    "def play_audio(audio_data_list):\n",
    "    for audio_data, sample_rate in audio_data_list:\n",
    "        print(f\"Playing audio with sample rate: {sample_rate} Hz\")\n",
    "        ipd.display(ipd.Audio(data=audio_data, rate=sample_rate))\n",
    "\n",
    "print(\"Real Audio\")\n",
    "play_audio(Real_Audio)\n",
    "print(\"Fake Audio\")\n",
    "play_audio(Fake_Audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h_6czCmrhAG"
   },
   "source": [
    "# *PREPROCESSING FUNCTION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available and set the device accordingly\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1732586434405,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "aeM3kWbZrDqH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Check MPS availability and set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def bandpass_filter(y, sr, lowcut=250, highcut=4000, order=5):\n",
    "    \"\"\"\n",
    "    Applies a bandpass filter to an audio signal.\n",
    "\n",
    "    Args:\n",
    "        y (torch.Tensor): The audio signal as a PyTorch tensor.\n",
    "        sr (int): The sample rate of the audio signal.\n",
    "        lowcut (int, optional): The lower cutoff frequency. Defaults to 250.\n",
    "        highcut (int, optional): The upper cutoff frequency. Defaults to 4000.\n",
    "        order (int, optional): The order of the filter. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The filtered audio signal as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    # Perform the filtering (this part uses scipy and will run on the CPU)\n",
    "    nyq = 0.5 * sr\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y_filtered = lfilter(b, a, y)  # Move to CPU for scipy\n",
    "\n",
    "    # Move the filtered signal back to the original device\n",
    "    return torch.tensor(y_filtered, dtype=y.dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732586435668,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "DB-gIQMss60B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def decrease_low_db(y, sr, threshold_db=-50, target_db=-80):\n",
    "    \"\"\"\n",
    "    Giảm độ lớn của các mẫu âm thanh dưới ngưỡng dB cho trước đến độ to mong muốn,\n",
    "    giữ nguyên thời gian của tín hiệu âm thanh.\n",
    "\n",
    "    :param y: Tín hiệu âm thanh (tensor)\n",
    "    :param sr: Tần số lấy mẫu (Hz)\n",
    "    :param threshold_db: Ngưỡng dB để xác định các mẫu cần giảm độ lớn (ví dụ: -40 dB)\n",
    "    :param target_db: Độ to mong muốn cho các mẫu dưới ngưỡng (ví dụ: -80 dB)\n",
    "    :return: Tín hiệu đã được điều chỉnh (tensor)\n",
    "    \"\"\"\n",
    "    # Calculate the absolute amplitude of the signal\n",
    "    abs_y = torch.abs(y)\n",
    "\n",
    "    # Calculate the reference amplitude (maximum amplitude)\n",
    "    ref_amplitude = torch.max(abs_y) if torch.max(abs_y) > 0 else torch.tensor(1.0, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Calculate the dB level of each sample relative to the reference amplitude\n",
    "    y_db = 20 * torch.log10(abs_y / ref_amplitude + 1e-10)  # Add epsilon to avoid log(0)\n",
    "\n",
    "    # Create a mask for samples below the dB threshold\n",
    "    mask = y_db < threshold_db\n",
    "\n",
    "    # Calculate the desired amplitude for samples below the dB threshold\n",
    "    desired_amplitude = 10 ** (target_db / 20) * ref_amplitude  # Example: -80 dB\n",
    "\n",
    "    # Create a copy of the signal to adjust\n",
    "    y_adjusted = y.clone()\n",
    "\n",
    "    # Reduce the amplitude of samples below the dB threshold\n",
    "    # Avoid division by zero by adding epsilon\n",
    "    y_adjusted[mask] = y_adjusted[mask] / (abs_y[mask] + 1e-10) * desired_amplitude\n",
    "\n",
    "    return y_adjusted  # Convert back to numpy array if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vvvdZeetmNg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MspLi1niuKFa"
   },
   "source": [
    "# SHOW PROCESSED INSTANCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CycaecXorgEO"
   },
   "outputs": [],
   "source": [
    "# Filtered_Real_Audio = []\n",
    "# for audio_data, sample_rate in Real_Audio:\n",
    "#     filtered_audio = bandpass_filter(audio_data, sample_rate, lowcut=250, highcut=4000)\n",
    "#     final_audio = decrease_low_db(filtered_audio, sample_rate)\n",
    "#     Filtered_Real_Audio.append(final_audio)\n",
    "\n",
    "# Filtered_Fake_Audio = []\n",
    "# for audio_data, sample_rate in Fake_Audio:\n",
    "#     filtered_audio = bandpass_filter(audio_data, sample_rate, lowcut=250, highcut=4000)\n",
    "#     filtered_audio = decrease_low_db(filtered_audio, sample_rate)\n",
    "#     Filtered_Fake_Audio.append(filtered_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rpqO-OR3xTuC",
    "outputId": "a286c7b2-0bc4-4fac-c848-1cb1033ff653"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def play_and_show_wave_spectrogram(audio_data, sample_rate, title):\n",
    "#     \"\"\"Plays audio, displays waveform, and spectrogram.\"\"\"\n",
    "#     ipd.display(ipd.Audio(data=audio_data, rate=sample_rate))  # Play audio\n",
    "\n",
    "#     # Display waveform\n",
    "#     # plt.figure(figsize=(10, 4))\n",
    "#     # librosa.display.waveshow(audio_data, sr=sample_rate)\n",
    "#     # plt.title(f\"{title} - Waveform\")\n",
    "#     # plt.xlabel(\"Time (s)\")\n",
    "#     # plt.ylabel(\"Amplitude\")\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n",
    "\n",
    "#     # Display spectrogram\n",
    "#     # plt.figure(figsize=(10, 4))\n",
    "#     # D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "#     # librosa.display.specshow(D, sr=sample_rate, x_axis='time', y_axis='log')\n",
    "#     # plt.colorbar(format='%+2.0f dB')\n",
    "#     # plt.title(f\"{title} - Spectrogram\")\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n",
    "\n",
    "# # Play, display waveform, and spectrogram for Filtered_Real_Audio\n",
    "# print(\"Real Audio Display\")\n",
    "# for i, audio_data in enumerate(Filtered_Real_Audio):\n",
    "#     play_and_show_wave_spectrogram(audio_data, Real_Audio[i][1], f\"Filtered Real Audio {i+1}\")\n",
    "\n",
    "# # Play, display waveform, and spectrogram for Filtered_Fake_Audio\n",
    "# print(\"Fake Audio Display\")\n",
    "# for i, audio_data in enumerate(Filtered_Fake_Audio):\n",
    "#     play_and_show_wave_spectrogram(audio_data, Fake_Audio[i][1], f\"Filtered Fake Audio {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV62zb5qtg-L"
   },
   "source": [
    "# TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732586540406,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "D92vGPt6bENR",
    "outputId": "8a266eb6-9b0e-4d64-dd04-d089c6d32dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file paths: 21120\n",
      "Test file paths: 5282\n"
     ]
    }
   ],
   "source": [
    "# Define the main data directory\n",
    "\n",
    "# List to hold file paths\n",
    "train_file_paths = []\n",
    "test_file_paths = []\n",
    "\n",
    "label_dict = {\n",
    "    fake_path: 0,\n",
    "    real_path: 1\n",
    "}\n",
    "\n",
    "# Split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Split files in each class directory\n",
    "for class_dir in [fake_path, real_path]:\n",
    "\n",
    "    # Get all file paths for the class\n",
    "    all_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.wav')]\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_files, test_files = train_test_split(all_files, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    # Append to the respective lists with corresponding labels (class)\n",
    "    for file_path in train_files:\n",
    "        train_file_paths.append((file_path, label_dict[class_dir]))  # Store path and label\n",
    "    for file_path in test_files:\n",
    "        test_file_paths.append((file_path, label_dict[class_dir]))\n",
    "\n",
    "print(f\"Train file paths: {len(train_file_paths)}\")\n",
    "print(f\"Test file paths: {len(test_file_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1732586580946,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "cJYk9AcdF8bO",
    "outputId": "cebbd6cb-685c-4ba7-d758-c85d329b8b42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file paths saved to output/train_file_paths.csv\n",
      "Test file paths saved to output/test_file_paths.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File paths to save the CSVs\n",
    "train_csv = 'output/train_file_paths.csv'\n",
    "test_csv = 'output/test_file_paths.csv'\n",
    "\n",
    "# Save train_file_paths to CSV\n",
    "with open(train_csv, mode='w', newline='') as train_file:\n",
    "    writer = csv.writer(train_file)\n",
    "    writer.writerow(['file_path', 'label'])  # Write the header\n",
    "    for file_path, label in train_file_paths:\n",
    "        writer.writerow([file_path, label])  # Write the file path and label\n",
    "\n",
    "# Save test_file_paths to CSV\n",
    "with open(test_csv, mode='w', newline='') as test_file:\n",
    "    writer = csv.writer(test_file)\n",
    "    writer.writerow(['file_path', 'label'])  # Write the header\n",
    "    for file_path, label in test_file_paths:\n",
    "        writer.writerow([file_path, label])  # Write the file path and label\n",
    "\n",
    "print(f\"Train file paths saved to {train_csv}\")\n",
    "print(f\"Test file paths saved to {test_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuvsA5gzF8bP"
   },
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1732588475361,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "lVT40_iHF8bP"
   },
   "outputs": [],
   "source": [
    "SEGMENT_LENGTH = 1\n",
    "NUM_SEGMENT = 30\n",
    "SR = 24000\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available and set the device accordingly\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1732586600090,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "WooQOFc5F8bP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# Check MPS availability and set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def segment_to_spectrogram(segment, sr=24000, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Extracts a Mel spectrogram from an audio segment, ensuring execution on the MPS GPU if available.\n",
    "\n",
    "    Args:\n",
    "        segment (torch.Tensor): The audio segment as a PyTorch tensor.\n",
    "        sr (int, optional): The sample rate of the audio segment. Defaults to 24000.\n",
    "        n_fft (int, optional): The size of the FFT. Defaults to 2048.\n",
    "        hop_length (int, optional): The hop length for the STFT. Defaults to 512.\n",
    "        n_mels (int, optional): The number of Mel filterbanks. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Mel spectrogram in decibels (dB).\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the MelSpectrogram transform\n",
    "    mel_spectrogram = T.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    ).to(device)  # Ensure the transform is also on the correct device\n",
    "\n",
    "    # Apply the MelSpectrogram transform\n",
    "    mel_spectrogram = mel_spectrogram(segment)\n",
    "\n",
    "    # Convert to decibels (dB)\n",
    "    spectrogram_db = T.AmplitudeToDB().to(device)  # Move AmplitudeToDB to the device\n",
    "    spectrogram_db = spectrogram_db(mel_spectrogram)\n",
    "\n",
    "    return spectrogram_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1370,
     "status": "ok",
     "timestamp": 1732586719872,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "F2ascfrZF8bP",
    "outputId": "9488f00d-7d2d-4637-cc3c-208264e83ce5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/d7p0zrqx7l3f7dwplj0g_b040000gn/T/ipykernel_41192/2508326885.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  waveform = torch.tensor(waveform, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 30 segments\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def extract_segments(audio_file, segment_length=SEGMENT_LENGTH, num_segments=NUM_SEGMENT):\n",
    "    # Load audio file using torchaudio\n",
    "    waveform, sr = torchaudio.load(audio_file)\n",
    "    waveform = bandpass_filter(waveform, sr, lowcut=250, highcut=4000)\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32).to(device)\n",
    "    waveform = decrease_low_db(waveform, sr)\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sr != SR:\n",
    "        resampler = T.Resample(orig_freq=sr, new_freq=SR)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = SR\n",
    "\n",
    "    # Calculate the total duration in seconds\n",
    "    total_duration = waveform.shape[1] / sr\n",
    "\n",
    "    # Calculate the overlap to ensure exactly num_segments\n",
    "    overlap = (total_duration - segment_length) / (num_segments - 1)\n",
    "\n",
    "    # Convert segment length and overlap to samples\n",
    "    segment_samples = int(segment_length * sr)\n",
    "    overlap_samples = int(overlap * sr)\n",
    "\n",
    "    # Extract the segments\n",
    "    segments = []\n",
    "    for i in range(num_segments):\n",
    "        start_sample = i * overlap_samples\n",
    "        end_sample = start_sample + segment_samples\n",
    "        segment = waveform[:, start_sample:end_sample]\n",
    "        spectrogram = segment_to_spectrogram(segment)\n",
    "        segments.append(spectrogram)\n",
    "\n",
    "    return segments\n",
    "\n",
    "# Example usage\n",
    "audio_file = './content/LibriSeVoc/gt/19_227_000003_000000.wav'\n",
    "segments = extract_segments(audio_file)\n",
    "print(f\"Extracted {len(segments)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1732588501369,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "DAywUfvsF8bP",
    "outputId": "9f83ba35-702e-413b-f582-f3747dbb7f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              file_path  label\n",
      "0     ./content/LibriSeVoc/gt/60_121082_000096_00000...      0\n",
      "1     ./content/LibriSeVoc/gt/8312_279790_000004_000...      0\n",
      "2     ./content/LibriSeVoc/gt/3168_173564_000017_000...      0\n",
      "3     ./content/LibriSeVoc/gt/7302_86814_000053_0000...      0\n",
      "4     ./content/LibriSeVoc/gt/4813_248638_000012_000...      0\n",
      "...                                                 ...    ...\n",
      "1995  ./content/LibriSeVoc/diffwave/1116_132851_0000...      1\n",
      "1996  ./content/LibriSeVoc/diffwave/87_121553_000086...      1\n",
      "1997  ./content/LibriSeVoc/diffwave/2002_139469_0000...      1\n",
      "1998  ./content/LibriSeVoc/diffwave/1088_129236_0000...      1\n",
      "1999  ./content/LibriSeVoc/diffwave/3168_173565_0000...      1\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV containing train and validation file paths and labels\n",
    "train_csv = './output/train_file_paths.csv'  # Path to the train data CSV\n",
    "\n",
    "train_data = pd.read_csv(train_csv)\n",
    "\n",
    "train_data_head = train_data.head(1000)\n",
    "train_data_tail = train_data.tail(1000)\n",
    "train_data_tail = train_data_tail.reset_index(drop=True)\n",
    "train_data_head = train_data_head.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Concatenate the head and tail along rows (axis=0)\n",
    "demo_train_data = pd.concat([train_data_head, train_data_tail], axis=0)\n",
    "demo_train_data = demo_train_data.reset_index(drop=True)\n",
    "\n",
    "# Display the merged data\n",
    "print(demo_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495854,
     "status": "ok",
     "timestamp": 1732588999288,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "1zsxc37kF8bQ",
    "outputId": "ecace7f8-593e-4aef-c51b-7986f039fd40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/d7p0zrqx7l3f7dwplj0g_b040000gn/T/ipykernel_41192/2261082929.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(SAVE_PATH + '.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments shape: (2000, 30, 128, 47, 1), Train labels shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = './output/train_data_checkpoint/train_data_partial'\n",
    "SAVE_INTERVAL = 100\n",
    "\n",
    "def load_data(data, segment_length=SEGMENT_LENGTH, num_segments=NUM_SEGMENT, save_interval=SAVE_INTERVAL, save_path=SAVE_PATH, start_index=0):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    # Check for existing partial files\n",
    "    partial_files = [f for f in os.listdir(os.path.dirname(save_path)) if f.startswith(os.path.basename(save_path)) and f.endswith('.pt')]\n",
    "    num_partial_files = len(partial_files)\n",
    "    if start_index == 0:\n",
    "        start_index = num_partial_files * save_interval\n",
    "    else:\n",
    "        start_index = start_index * save_interval\n",
    "    print(f\"Resuming from index {start_index}\")\n",
    "\n",
    "    if num_partial_files * save_interval >= len(data):\n",
    "        print(\"Already finished processing. Merging.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"Continuing processing from the last checkpoint.\")\n",
    "\n",
    "    for idx, (_, row) in tqdm(enumerate(data.iterrows()), total=len(data)):\n",
    "        if idx < start_index:\n",
    "            continue\n",
    "\n",
    "        file_path = row['file_path']\n",
    "        label = row['label']\n",
    "\n",
    "        # Extract segments from the audio file\n",
    "        file_segments = extract_segments(file_path, segment_length, num_segments)\n",
    "\n",
    "        # Stack the segments and convert to torch tensor\n",
    "        file_segments_stacked = torch.stack([torch.tensor(segment, dtype=torch.float32) for segment in file_segments])\n",
    "\n",
    "        # Append the stacked segments and their corresponding labels\n",
    "        segments.append(file_segments_stacked)  # Append the 30 segments as a single element\n",
    "        labels.append(label)  # Append the label only once per file\n",
    "\n",
    "        # Save progress every save_interval\n",
    "        if (idx + 1) % save_interval == 0 or (idx + 1) == len(data):\n",
    "            partial_path = f\"{save_path}_{int((idx + 1) / save_interval)}.pt\"\n",
    "            torch.save({'segments': segments, 'labels': labels}, partial_path)\n",
    "            print(f\"{len(segments)} saved at index {idx + 1} to {partial_path}\")  # More informative print\n",
    "            segments = []\n",
    "            labels = []\n",
    "\n",
    "    print(\"All segments saved individually.\")\n",
    "\n",
    "def load_saved_segments(data, save_path=SAVE_PATH):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Load all partial files in the SAVE_PATH subdir\n",
    "    partial_files = [f for f in os.listdir(os.path.dirname(save_path)) if f.startswith(os.path.basename(save_path)) and f.endswith('.pt')]\n",
    "    for partial_file in partial_files:\n",
    "        print(f\"Loading {partial_file}\")\n",
    "        partial_data = torch.load(os.path.join(os.path.dirname(save_path), partial_file))\n",
    "        print(f\"Loaded {len(partial_data['segments'])} segments from {partial_file}\")\n",
    "        all_segments.extend(partial_data['segments'])  # Extend directly with the list of stacked segments\n",
    "        all_labels.extend(partial_data['labels'])  # Extend the labels list\n",
    "\n",
    "    return torch.stack([segment.to(device) for segment in all_segments]), torch.tensor(all_labels, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "if os.path.exists(SAVE_PATH + '.pt'):\n",
    "    # Load train and validation data\n",
    "    data = torch.load(SAVE_PATH + '.pt')\n",
    "    train_segments = data['segments']\n",
    "    train_labels = data['labels']\n",
    "else:\n",
    "    # Load the train and validation data\n",
    "    load_data(demo_train_data)  # Save segments individually\n",
    "    train_segments, train_labels = load_saved_segments(demo_train_data)  # Load saved segments\n",
    "    print(\"Completed loading train data. Saving...\")\n",
    "    # Save the train and validation data\n",
    "    torch.save({'segments': train_segments, 'labels': train_labels}, f'{SAVE_PATH}.pt')\n",
    "\n",
    "train_segments = np.array([segment.cpu().numpy() for segment in train_segments])\n",
    "train_segments = np.transpose(train_segments, (0, 1, 3, 4, 2))\n",
    "train_labels = train_labels.cpu().numpy()\n",
    "print(f\"Train segments shape: {train_segments.shape}, Train labels shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1862,
     "status": "ok",
     "timestamp": 1732589001147,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "i1udxkc3F8bQ",
    "outputId": "e5cd1344-fb99-4d23-bfbd-82a4873aad07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train segments shape: (2000, 30, 128, 47, 1), Train labels shape: (1600,)\n",
      "Validation segments shape: (400, 30, 128, 47, 1), Validation labels shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "# Split the train data further into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_segments, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train segments shape: {train_segments.shape}, Train labels shape: {y_train.shape}\")\n",
    "print(f\"Validation segments shape: {X_val.shape}, Validation labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWbTuEvNF8bQ"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1732588358346,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "sXiLYB7NF8bQ",
    "outputId": "f23345d4-6a45-4ca5-871c-0f053277935d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronan/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/ronan/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "2024-11-29 15:23:46.506243: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2024-11-29 15:23:46.506266: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-11-29 15:23:46.506269: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-11-29 15:23:46.506282: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-29 15:23:46.506297: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), input_shape=input_shape, padding='same', kernel_initializer=initializer),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), padding='same', kernel_initializer=initializer),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), padding='same', kernel_initializer=initializer),\n",
    "        layers.LeakyReLU(alpha=0.1),\n",
    "        layers.GlobalAveragePooling2D(),  # Outputs a 1D feature vector for each segment\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Assume input_shape is (num_freq_bins, time_steps, 1)\n",
    "input_shape = (128, 47, 1)  # Example input shape based on typical spectrogram size\n",
    "cnn_model = create_cnn_model(input_shape)\n",
    "\n",
    "# Input for multiple segments\n",
    "num_segments = NUM_SEGMENT  # Example number of segments per audio file\n",
    "segment_input = Input(shape=(num_segments, *input_shape))\n",
    "\n",
    "# Apply CNN model to each segment\n",
    "cnn_features = layers.TimeDistributed(cnn_model)(segment_input)  # Shape: (batch, num_segments, feature_dim)\n",
    "# Add LSTM layers (batch_size, timesteps, input_dim)\n",
    "lstm_layer = layers.LSTM(128, return_sequences=True)(cnn_features)\n",
    "# lstm_layer = layers.BatchNormalization()(lstm_layer)\n",
    "lstm_layer = layers.Dropout(0.3)(lstm_layer)\n",
    "lstm_layer = layers.LSTM(128, return_sequences=True)(lstm_layer)\n",
    "# lstm_layer = layers.BatchNormalization()(lstm_layer)\n",
    "lstm_layer = layers.Dropout(0.3)(lstm_layer)\n",
    "lstm_layer = layers.LSTM(128, return_sequences=False)(lstm_layer)\n",
    "\n",
    "# Classification Layer\n",
    "x = layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(lstm_layer)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=segment_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">92,672</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m1\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m92,672\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">586,369</span> (2.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m586,369\u001b[0m (2.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">586,369</span> (2.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m586,369\u001b[0m (2.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell nay Nam them vao\n",
    "\n",
    "initial_learning_rate = 0.0001 # Your initial learning rate\n",
    "decay_steps = 10000  # Number of steps to decay over\n",
    "decay_rate = 0.9  # Decay rate\n",
    "\n",
    "# lr_schedule = ExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=decay_steps,\n",
    "#     decay_rate=decay_rate,\n",
    "#     staircase=True  # Use staircase decay (optional)\n",
    "# )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1732589054217,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "IHZadyWRVoP3"
   },
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, clipvalue=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1732589103930,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "bs7g9q9PVrUy"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w2SMATuVitx"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1732589143643,
     "user": {
      "displayName": "Tuấn Nam Đỗ",
      "userId": "12452333077463821897"
     },
     "user_tz": -420
    },
    "id": "M2Z-8xnDU93V"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "MODEL_PATH = './Nam_output/best_model.keras'\n",
    "HISTORY_PATH = './Nam_output/train_history.json'\n",
    "\n",
    "class SaveHistory(Callback):\n",
    "    def __init__(self, filepath=HISTORY_PATH):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Save the history to a JSON file\n",
    "        if os.path.exists(self.filepath):\n",
    "            with open(self.filepath, 'r') as f:\n",
    "                history = json.load(f)\n",
    "        else:\n",
    "            history = {}\n",
    "\n",
    "        for key, value in logs.items():\n",
    "            if key in history:\n",
    "                history[key].append(value)\n",
    "            else:\n",
    "                history[key] = [value]\n",
    "\n",
    "        with open(self.filepath, 'w') as f:\n",
    "            json.dump(history, f)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=MODEL_PATH,  # Save weights with epoch number\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,  # Save weights every epoch, not just the best\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "save_history_callback = SaveHistory()\n",
    "callbacks = [checkpoint_callback, early_stopping, save_history_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 30, 128, 47, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqcUaPGXF8bQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 15:23:48.606710: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5411 - loss: 1.0090\n",
      "Epoch 1: val_loss improved from inf to 0.99952, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 8s/step - accuracy: 0.5432 - loss: 1.0088 - val_accuracy: 0.6225 - val_loss: 0.9995\n",
      "Epoch 2/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.6270 - loss: 0.9952\n",
      "Epoch 2: val_loss improved from 0.99952 to 0.97557, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 5s/step - accuracy: 0.6280 - loss: 0.9947 - val_accuracy: 0.6425 - val_loss: 0.9756\n",
      "Epoch 3/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6675 - loss: 0.9685\n",
      "Epoch 3: val_loss improved from 0.97557 to 0.97440, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.6667 - loss: 0.9680 - val_accuracy: 0.5900 - val_loss: 0.9744\n",
      "Epoch 4/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6216 - loss: 0.9544\n",
      "Epoch 4: val_loss improved from 0.97440 to 0.94658, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.6238 - loss: 0.9531 - val_accuracy: 0.6225 - val_loss: 0.9466\n",
      "Epoch 5/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6666 - loss: 0.9281\n",
      "Epoch 5: val_loss did not improve from 0.94658\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6663 - loss: 0.9278 - val_accuracy: 0.6075 - val_loss: 0.9469\n",
      "Epoch 6/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6617 - loss: 0.9199\n",
      "Epoch 6: val_loss improved from 0.94658 to 0.93343, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.6625 - loss: 0.9195 - val_accuracy: 0.6525 - val_loss: 0.9334\n",
      "Epoch 7/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6755 - loss: 0.9075\n",
      "Epoch 7: val_loss improved from 0.93343 to 0.92343, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.6767 - loss: 0.9067 - val_accuracy: 0.6525 - val_loss: 0.9234\n",
      "Epoch 8/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6733 - loss: 0.8932\n",
      "Epoch 8: val_loss improved from 0.92343 to 0.91304, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.6732 - loss: 0.8934 - val_accuracy: 0.6550 - val_loss: 0.9130\n",
      "Epoch 9/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6685 - loss: 0.8973\n",
      "Epoch 9: val_loss improved from 0.91304 to 0.90294, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.6688 - loss: 0.8967 - val_accuracy: 0.6675 - val_loss: 0.9029\n",
      "Epoch 10/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6828 - loss: 0.8865\n",
      "Epoch 10: val_loss improved from 0.90294 to 0.89026, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6839 - loss: 0.8854 - val_accuracy: 0.6825 - val_loss: 0.8903\n",
      "Epoch 11/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6815 - loss: 0.8795\n",
      "Epoch 11: val_loss did not improve from 0.89026\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.6827 - loss: 0.8787 - val_accuracy: 0.6550 - val_loss: 0.8910\n",
      "Epoch 12/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6899 - loss: 0.8695\n",
      "Epoch 12: val_loss improved from 0.89026 to 0.88180, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.6903 - loss: 0.8692 - val_accuracy: 0.6800 - val_loss: 0.8818\n",
      "Epoch 13/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6985 - loss: 0.8650\n",
      "Epoch 13: val_loss improved from 0.88180 to 0.87477, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.6979 - loss: 0.8653 - val_accuracy: 0.6875 - val_loss: 0.8748\n",
      "Epoch 14/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6824 - loss: 0.8635\n",
      "Epoch 14: val_loss improved from 0.87477 to 0.86619, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.6819 - loss: 0.8641 - val_accuracy: 0.6925 - val_loss: 0.8662\n",
      "Epoch 15/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7018 - loss: 0.8482\n",
      "Epoch 15: val_loss improved from 0.86619 to 0.85528, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.7007 - loss: 0.8489 - val_accuracy: 0.7050 - val_loss: 0.8553\n",
      "Epoch 16/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6909 - loss: 0.8510\n",
      "Epoch 16: val_loss improved from 0.85528 to 0.84993, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.6907 - loss: 0.8509 - val_accuracy: 0.7125 - val_loss: 0.8499\n",
      "Epoch 17/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6978 - loss: 0.8464\n",
      "Epoch 17: val_loss did not improve from 0.84993\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3s/step - accuracy: 0.6985 - loss: 0.8455 - val_accuracy: 0.6900 - val_loss: 0.8741\n",
      "Epoch 18/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7059 - loss: 0.8357\n",
      "Epoch 18: val_loss improved from 0.84993 to 0.84607, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.7053 - loss: 0.8366 - val_accuracy: 0.7100 - val_loss: 0.8461\n",
      "Epoch 19/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6976 - loss: 0.8489\n",
      "Epoch 19: val_loss improved from 0.84607 to 0.83338, saving model to ./Nam_output/best_model.keras\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 0.6968 - loss: 0.8482 - val_accuracy: 0.7050 - val_loss: 0.8334\n",
      "Epoch 20/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7138 - loss: 0.8275\n",
      "Epoch 20: val_loss did not improve from 0.83338\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.7137 - loss: 0.8277 - val_accuracy: 0.7025 - val_loss: 0.8452\n",
      "Epoch 21/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6950 - loss: 0.8367\n",
      "Epoch 21: val_loss did not improve from 0.83338\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3s/step - accuracy: 0.6952 - loss: 0.8366 - val_accuracy: 0.6450 - val_loss: 0.8995\n",
      "Epoch 22/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7038 - loss: 0.8338\n",
      "Epoch 22: val_loss did not improve from 0.83338\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.7039 - loss: 0.8339 - val_accuracy: 0.7000 - val_loss: 0.8489\n",
      "Epoch 23/200\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6726 - loss: 0.8532\n",
      "Epoch 23: val_loss did not improve from 0.83338\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step - accuracy: 0.6749 - loss: 0.8515 - val_accuracy: 0.6400 - val_loss: 0.8548\n",
      "Epoch 24/200\n",
      "\u001b[1m2/7\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 10s/step - accuracy: 0.7080 - loss: 0.8191 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m         initial_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResuming from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_history:\n\u001b[1;32m     17\u001b[0m     history\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;241m+\u001b[39m old_history[key] \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/Developer/deepfake-audio-detector/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "old_history = None\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model = models.load_model(MODEL_PATH)\n",
    "    print(model)\n",
    "if os.path.exists(HISTORY_PATH):\n",
    "    with open(HISTORY_PATH, 'r') as f:\n",
    "        history = json.load(f)\n",
    "        print(history)\n",
    "        initial_epoch = len(history['loss'])\n",
    "        print(f\"Resuming from epoch {initial_epoch}\")\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=256, shuffle=True, validation_data=(X_val, y_val), callbacks=callbacks, initial_epoch=initial_epoch)\n",
    "\n",
    "if old_history:\n",
    "    history.history = {key: value + old_history[key] for key, value in history.history.items()}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
